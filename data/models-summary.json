{
  "lastUpdated": "2025-10-08T22:51:54.340Z",
  "totalModels": 182,
  "totalTags": 6760,
  "categories": [
    "base",
    "chat",
    "code",
    "instruct",
    "latest",
    "thinking",
    "tools",
    "uncensored",
    "vision"
  ],
  "topModels": [
    {
      "name": "llama3.1",
      "pullCount": "103.9M",
      "description": "Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.",
      "tagCount": 93
    },
    {
      "name": "deepseek-r1",
      "pullCount": "65M",
      "description": "DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro.",
      "tagCount": 35
    },
    {
      "name": "nomic-embed-text",
      "pullCount": "42.4M",
      "description": "A high-performing open embedding model with a large token context window.",
      "tagCount": 3
    },
    {
      "name": "llama3.2",
      "pullCount": "38.9M",
      "description": "Meta's Llama 3.2 goes small with 1B and 3B models.",
      "tagCount": 63
    },
    {
      "name": "mistral",
      "pullCount": "20.5M",
      "description": "The 7B model released by Mistral AI, updated to version 0.3.",
      "tagCount": 84
    },
    {
      "name": "gemma3",
      "pullCount": "19.9M",
      "description": "The current, most capable model that runs on a single GPU.",
      "tagCount": 26
    },
    {
      "name": "qwen2.5",
      "pullCount": "14.8M",
      "description": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
      "tagCount": 133
    },
    {
      "name": "llama3",
      "pullCount": "11.2M",
      "description": "Meta Llama 3: The most capable openly available LLM to date",
      "tagCount": 68
    },
    {
      "name": "phi3",
      "pullCount": "11.1M",
      "description": "Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.",
      "tagCount": 72
    },
    {
      "name": "llava",
      "pullCount": "10.3M",
      "description": "ðŸŒ‹ LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6.",
      "tagCount": 98
    },
    {
      "name": "qwen3",
      "pullCount": "9.6M",
      "description": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.",
      "tagCount": 58
    },
    {
      "name": "gemma2",
      "pullCount": "8.1M",
      "description": "Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.",
      "tagCount": 94
    },
    {
      "name": "qwen2.5-coder",
      "pullCount": "7.5M",
      "description": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.",
      "tagCount": 199
    },
    {
      "name": "gemma",
      "pullCount": "5.3M",
      "description": "Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1",
      "tagCount": 102
    },
    {
      "name": "phi4",
      "pullCount": "5.3M",
      "description": "Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.",
      "tagCount": 5
    },
    {
      "name": "mxbai-embed-large",
      "pullCount": "5M",
      "description": "State-of-the-art large embedding model from mixedbread.ai",
      "tagCount": 4
    },
    {
      "name": "qwen",
      "pullCount": "4.9M",
      "description": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
      "tagCount": 379
    },
    {
      "name": "qwen2",
      "pullCount": "4.4M",
      "description": "Qwen2 is a new series of large language models from Alibaba group",
      "tagCount": 97
    },
    {
      "name": "llama2",
      "pullCount": "4.2M",
      "description": "Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.",
      "tagCount": 102
    },
    {
      "name": "minicpm-v",
      "pullCount": "3.6M",
      "description": "A series of multimodal LLMs (MLLMs) designed for vision-language understanding.",
      "tagCount": 17
    }
  ]
}